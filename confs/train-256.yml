shuffle: True
lr: 1e-4  # 1e-4
weight_decay: 0.0
lr_anneal_steps: 0
batch_size: 8
microbatch: -1  # -1 disables microbatches
ema_rate: 0.9999  # comma-separated list of EMA values
log_interval: 100
save_interval: 10000
checkpoint_dir: ./checkpoint/tea_256_free/
resume_checkpoint:   # checkpoint 
use_fp16: true  # 启用混合精度训练，启用混合精度训练，使用PyTorch官方的自动混合精度(AMP)功能 
fp16_scale_growth: 1e-3
sample_num: 4
# diffussion
image_size: 256
num_channels: 128  # 256 in diffusion model 
class_cond: false   # class generation
learn_sigma: true
diffusion_steps: 1000
noise_schedule: linear    ################ 
schedule_sampler: uniform
timestep_respacing: ""  # dont set when training
use_kl: False
predict_xstart: False
rescale_timesteps: False
rescale_learned_sigmas: False
clip_denoised: true
respace_interpolate: false # Not Implemented
num_res_blocks: 2
num_heads: 4
num_heads_upsample: -1
num_head_channels: 32
attention_resolutions: 16,8
channel_mult: "1, 1, 2, 3, 4, 4"
dropout: 0.0
use_checkpoint: False
use_scale_shift_norm: True
resblock_updown: true
use_new_attention_order: False
class_json_path: 

# classifier train
classifier_train_batch_size: 64
classifier_noised: True
classifier_iterations: 300000
classifier_lr: 3e-4
classifier_weight_decay: 0.05
classifier_anneal_lr: True
classifier_checkpoint: ""
classifier_resume_checkpoint: ""
classifier_log_interval: 100
classifier_save_interval: 10000

# classifier inference
classifier_scale: 4.0
lr_kernel_n_std: 2
num_samples: 100
show_progress: true  # stdm
classifier_use_fp16: false
classifier_width: 128   # in classifier model
classifier_depth: 2
classifier_attention_resolutions: 32,16,8
classifier_use_scale_shift_norm: true
classifier_resblock_updown: true
classifier_pool: attention
classifier_path: ''
